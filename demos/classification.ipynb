{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1b7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path(\"..\").resolve()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDT\n",
    "\n",
    "from arboresque import DecisionTreeClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5178d",
   "metadata": {},
   "source": [
    "The iris dataset contains 150 samples of sepal and petal measurement data, 4 features in all, for three types of irises, Setosa, Versicolor and Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0945f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8 3.  1.4 0.1] 0\n",
      "[4.3 3.  1.1 0.1] 0\n",
      "[6.7 3.1 4.7 1.5] 1\n",
      "[6.7 3.1 5.6 2.4] 2\n",
      "[6.6 3.  4.4 1.4] 1\n"
     ]
    }
   ],
   "source": [
    "for i in [12, 13, 86, 140, 75]:\n",
    "    print(iris.data[i], iris.target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09d9f404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris classifier\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier()  # default criterion=\"gini\"\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Iris classifier\")\n",
    "print(\"Train accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d51fd46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.79      1.00      0.88        15\n",
      "           2       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.93      0.91      0.91        45\n",
      "weighted avg       0.93      0.91      0.91        45\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[13  2  0]\n",
      " [ 0 15  0]\n",
      " [ 0  2 13]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "074a38fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: gini\n",
      "    Train accuracy: 1.000\n",
      "    Test accuracy:  0.911\n",
      "    Time: 0.026\n",
      "    Depth: 5\n",
      "\n",
      "Criterion: entropy\n",
      "    Train accuracy: 1.000\n",
      "    Test accuracy:  0.844\n",
      "    Time: 0.010\n",
      "    Depth: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterions = [\"gini\", \"entropy\"]\n",
    "\n",
    "results = []\n",
    "for crit in criterions:\n",
    "    start = time.time()\n",
    "    clf = DecisionTreeClassifier(criterion=crit)\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    tm = end-start\n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    test_acc = clf.score(X_test, y_test)\n",
    "    print(f\"Criterion: {crit}\")\n",
    "    print(f\"    Train accuracy: {train_acc:.3f}\")\n",
    "    print(f\"    Test accuracy:  {test_acc:.3f}\")\n",
    "    print(f\"    Time: {tm:.3f}\")\n",
    "    print(f\"    Depth: {clf.get_depth()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04358ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c83e1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample true labels:  [2 2 0 0 1]\n",
      "Predicted labels:    [2 2 0 1 1]\n",
      "Predicted probs (rows):\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Row sums (should be 1): [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_sample = X_test[:5]\n",
    "y_sample = y_test[:5]\n",
    "\n",
    "probs = clf.predict_proba(X_sample)\n",
    "preds = clf.predict(X_sample)\n",
    "\n",
    "print(\"Sample true labels: \", y_sample)\n",
    "print(\"Predicted labels:   \", preds)\n",
    "print(\"Predicted probs (rows):\")\n",
    "print(probs)\n",
    "print(\"Row sums (should be 1):\", probs.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2349396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: max_features variations\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     10\u001b[0m     acc \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_feat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf\u001b[38;5;241m.\u001b[39mget_depth()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Leaves=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_n_leaves\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest 2: min_samples_split & min_samples_leaf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32m~\\decision-trees\\arboresque\\tree.py:225\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.get_n_leaves\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_n_leaves\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_n_leaves\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\decision-trees\\arboresque\\_tree.py:47\u001b[0m, in \u001b[0;36mTree.get_n_leaves\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m q \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot]\n\u001b[0;32m     46\u001b[0m n_leaves \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m q:\n\u001b[0;32m     48\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(q)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(l):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Test 1: max_features variations\")\n",
    "print()\n",
    "\n",
    "for max_feat in [None, 2, 0.5, 'sqrt', 'log2']:\n",
    "    clf = DecisionTreeClassifier(max_features=max_feat, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(f\"max_features={max_feat}: Accuracy={acc:.3f}, Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "print(\"Test 2: min_samples_split & min_samples_leaf\")\n",
    "print()\n",
    "\n",
    "for min_split, min_leaf in [(2, 1), (10, 5), (20, 10), (0.1, 0.05)]:\n",
    "    clf = DecisionTreeClassifier(min_samples_split=min_split, min_samples_leaf=min_leaf, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(f\"min_split={min_split}, min_leaf={min_leaf}: Acc={acc:.3f}, Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "print(\"Test 3: min_impurity_decrease\")\n",
    "print()\n",
    "\n",
    "for min_imp in [0.0, 0.01, 0.05, 0.1]:\n",
    "    clf = DecisionTreeClassifier(min_impurity_decrease=min_imp, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(f\"min_impurity_decrease={min_imp}: Acc={acc:.3f}, Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "print(\"Test 4: random_state reproducibility\")\n",
    "print()\n",
    "\n",
    "clf1 = DecisionTreeClassifier(max_features=2, random_state=42)\n",
    "clf1.fit(X_train, y_train)\n",
    "pred1 = clf1.predict(X_test)\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_features=2, random_state=42)\n",
    "clf2.fit(X_train, y_train)\n",
    "pred2 = clf2.predict(X_test)\n",
    "\n",
    "print(f\"Same random_state: Predictions identical? {np.array_equal(pred1, pred2)}\")\n",
    "\n",
    "clf3 = DecisionTreeClassifier(max_features=2, random_state=99)\n",
    "clf3.fit(X_train, y_train)\n",
    "pred3 = clf3.predict(X_test)\n",
    "\n",
    "print(f\"Different random_state: Predictions different? {not np.array_equal(pred1, pred3)}\")\n",
    "\n",
    "print(\"Test 5: Categorical features\")\n",
    "print()\n",
    "\n",
    "# Create dataset with categorical feature\n",
    "X_cat = np.column_stack([X, np.random.randint(0, 3, size=len(X))])  # Add categorical column\n",
    "X_train_cat, X_test_cat, y_train, y_test = train_test_split(X_cat, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(categorical_features=[4], random_state=42)\n",
    "clf.fit(X_train_cat, y_train)\n",
    "acc = clf.score(X_test_cat, y_test)\n",
    "print(f\"With categorical feature at index 4: Accuracy={acc:.3f}\")\n",
    "print(f\"Features after encoding: {clf.n_features} (was {X_cat.shape[1]})\")\n",
    "\n",
    "print(\"Test 6: Comparison with sklearn\")\n",
    "print()\n",
    "\n",
    "X, y = wine.data, wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "your_clf = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "your_clf.fit(X_train, y_train)\n",
    "your_acc = your_clf.score(X_test, y_test)\n",
    "\n",
    "sklearn_clf = SklearnDT(\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "sklearn_clf.fit(X_train, y_train)\n",
    "sklearn_acc = sklearn_clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Your implementation: Acc={your_acc:.3f}, Depth={your_clf.get_depth()}, Leaves={your_clf.get_n_leaves()}\")\n",
    "print(f\"Sklearn:           Acc={sklearn_acc:.3f}, Depth={sklearn_clf.get_depth()}, Leaves={sklearn_clf.get_n_leaves()}\")\n",
    "print(f\"Accuracy difference: {abs(your_acc - sklearn_acc):.3f}\")\n",
    "\n",
    "print(\"Test 7: Edge cases\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Very restrictive parameters\n",
    "clf = DecisionTreeClassifier(max_depth=1, min_samples_split=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Very restrictive tree: Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "# Single sample per leaf\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=1, min_samples_split=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Max flexibility tree: Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "print(\"Test 8: predict_proba\")\n",
    "print()\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "proba = clf.predict_proba(X_test[:5])\n",
    "preds = clf.predict(X_test[:5])\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Predicted class: {preds[i]}, Probabilities: {proba[i]}\")\n",
    "    print(f\"  Sum of probabilities: {proba[i].sum():.3f}\")\n",
    "\n",
    "print(\"All tests completed.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca56acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris sklearn DecisionTreeClassifier (default gini)\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.9777777777777777\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       0.94      1.00      0.97        15\n",
      "           2       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  1 14]]\n",
      "\n",
      "Criterion: gini\n",
      "  Train accuracy: 1.0\n",
      "  Test accuracy:  0.978\n",
      "  Fit time (s):   0.0\n",
      "  Depth: 4\n",
      "\n",
      "Criterion: entropy\n",
      "  Train accuracy: 1.0\n",
      "  Test accuracy:  0.956\n",
      "  Fit time (s):   0.0\n",
      "  Depth: 7\n"
     ]
    }
   ],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "clf = SklearnDT(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Iris sklearn DecisionTreeClassifier (default gini)\")\n",
    "print(\"Train accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from time import time\n",
    "\n",
    "for crit in [\"gini\", \"entropy\"]:\n",
    "    t0 = time()\n",
    "    clf = SklearnDT(criterion=crit, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    t1 = time()\n",
    "    print(f\"\\nCriterion: {crit}\")\n",
    "    print(\"  Train accuracy:\", round(clf.score(X_train, y_train), 3))\n",
    "    print(\"  Test accuracy: \", round(clf.score(X_test, y_test), 3))\n",
    "    print(\"  Fit time (s):  \", round(t1 - t0, 6))\n",
    "    print(f\"  Depth: {clf.get_depth()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
